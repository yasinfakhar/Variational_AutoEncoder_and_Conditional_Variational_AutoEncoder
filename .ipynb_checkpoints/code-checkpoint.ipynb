{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4254eb65",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder\n",
    "## creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a75491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 784])\n",
      "torch.Size([4, 20])\n",
      "torch.Size([4, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasin/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=200, z_dim=20):\n",
    "        super().__init__()\n",
    "        # encoder\n",
    "        self.img_2hid = nn.Linear(input_dim, h_dim)\n",
    "        self.hid_2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hid_2sigma = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.z_2hid = nn.Linear(z_dim, h_dim)\n",
    "        self.hid_2img = nn.Linear(h_dim, input_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.relu(self.img_2hid(x))\n",
    "        mu, sigma = self.hid_2mu(h), self.hid_2sigma(h)\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.z_2hid(z))\n",
    "        return torch.sigmoid(self.hid_2img(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_new = mu + sigma*epsilon\n",
    "        x_reconstructed = self.decode(z_new)\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        return x_reconstructed, mu, sigma\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(4, 28*28)\n",
    "    vae = VariationalAutoEncoder(input_dim=784)\n",
    "    x_reconstructed, mu, sigma = vae(x)\n",
    "    print(x_reconstructed.shape)\n",
    "    print(mu.shape)\n",
    "    print(sigma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214895b8",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f46915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_DIM = 784\n",
    "H_DIM = 200\n",
    "Z_DIM = 20\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR_RATE = 2e-4  # Karpathy constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82702a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = VariationalAutoEncoder(INPUT_DIM, H_DIM, Z_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "loss_fn = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb7a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [00:26, 71.08it/s, loss=5.65e+3]\n",
      "1875it [00:26, 70.60it/s, loss=5.08e+3]\n",
      "1875it [00:26, 70.79it/s, loss=4.88e+3]\n",
      "1875it [00:26, 71.30it/s, loss=4.56e+3]\n",
      "1875it [00:26, 71.02it/s, loss=4.36e+3]\n",
      "1875it [00:26, 70.80it/s, loss=4.22e+3]\n",
      "1875it [00:26, 70.53it/s, loss=4.52e+3]\n",
      "1875it [00:27, 68.53it/s, loss=4.25e+3]\n",
      "1875it [00:26, 70.13it/s, loss=4.28e+3]\n",
      "1875it [00:26, 70.46it/s, loss=4.07e+3]\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(enumerate(train_loader))\n",
    "    for i, (x, _) in loop:\n",
    "        # Forward pass\n",
    "        x = x.to(DEVICE).view(x.shape[0], INPUT_DIM)\n",
    "        x_reconstructed, mu, sigma = model(x)\n",
    "\n",
    "        # Compute loss\n",
    "        reconstruction_loss = loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "\n",
    "        # Backprop\n",
    "        loss = reconstruction_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "torch.save(model.state_dict(), \"VAE_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b882334",
   "metadata": {},
   "source": [
    "## loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8743f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VariationalAutoEncoder(INPUT_DIM, H_DIM, Z_DIM).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"VAE_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a78db8",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa012a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "def inference(digit, num_examples=1):\n",
    "    \"\"\"\n",
    "    Generates (num_examples) of a particular digit.\n",
    "    Specifically we extract an example of each digit,\n",
    "    then after we have the mu, sigma representation for\n",
    "    each digit we can sample from that.\n",
    "\n",
    "    After we sample we can run the decoder part of the VAE\n",
    "    and generate examples.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    idx = 0\n",
    "    for x, y in dataset:\n",
    "        if y == idx:\n",
    "            images.append(x)\n",
    "            idx += 1\n",
    "        if idx == 10:\n",
    "            break\n",
    "\n",
    "    encodings_digit = []\n",
    "    for d in range(10):\n",
    "        with torch.no_grad():\n",
    "            mu, sigma = model.encode(images[d].view(1, 784))\n",
    "        encodings_digit.append((mu, sigma))\n",
    "\n",
    "    mu, sigma = encodings_digit[digit]\n",
    "    for example in range(num_examples):\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z = mu + sigma * epsilon\n",
    "        out = model.decode(z)\n",
    "        out = out.view(-1, 1, 28, 28)\n",
    "        save_image(out, f\"generated_{digit}_ex{example}.png\")\n",
    "\n",
    "for idx in range(10):\n",
    "    inference(idx, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f360a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "idx = 0\n",
    "for x, y in dataset:\n",
    "    if y == idx:\n",
    "        images.append(x)\n",
    "        idx += 1\n",
    "        if idx == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a22715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which number you want to generate ? 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_input = int(input('which number you want to generate ? '))\n",
    "model = model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    mu, sigma = model.encode(images[num_input].view(1, 784))\n",
    "epsilon = torch.randn_like(sigma)\n",
    "z = mu + sigma * epsilon\n",
    "out = model.decode(z)\n",
    "out = out.view(-1, 1, 28, 28)\n",
    "plt.imshow(out.cpu().detach()[0][0] , cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b126ad3",
   "metadata": {},
   "source": [
    "# Conditional Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc27f1a",
   "metadata": {},
   "source": [
    "## creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "335e3273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 794])\n",
      "torch.Size([4, 20])\n",
      "torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ConditionalVariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=200, z_dim=20):\n",
    "        super().__init__()\n",
    "        # encoder\n",
    "        self.img_2hid = nn.Linear(input_dim, h_dim)\n",
    "        self.hid_2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hid_2sigma = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.z_2hid = nn.Linear(z_dim, h_dim)\n",
    "        self.hid_2img = nn.Linear(h_dim, input_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.relu(self.img_2hid(x))\n",
    "        mu, sigma = self.hid_2mu(h), self.hid_2sigma(h)\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.z_2hid(z))\n",
    "        return torch.sigmoid(self.hid_2img(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_new = mu + sigma*epsilon\n",
    "        x_reconstructed = self.decode(z_new)\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        return x_reconstructed, mu, sigma\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(4, 28*28 + 10)\n",
    "    cvae = ConditionalVariationalAutoEncoder(input_dim=28*28 + 10)\n",
    "    x_reconstructed, mu, sigma = cvae(x)\n",
    "    print(x_reconstructed.shape)\n",
    "    print(mu.shape)\n",
    "    print(sigma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978cde6",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46845403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_DIM = 28*28 + 10\n",
    "H_DIM = 200\n",
    "Z_DIM = 20\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR_RATE = 2e-4  # Karpathy constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec01e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = ConditionalVariationalAutoEncoder(INPUT_DIM, H_DIM, Z_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "loss_fn = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ccf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(enumerate(train_loader))\n",
    "    for i, (x, _) in loop:\n",
    "        # Forward pass\n",
    "        x = x.to(DEVICE).view(x.shape[0], INPUT_DIM)\n",
    "        condition = F.one_hot(x, num_classes=10)\n",
    "        x_with_condition = torch.cat((x, condition), 1)\n",
    "        x_reconstructed, mu, sigma = model(x_with_condition)\n",
    "\n",
    "        # Compute loss\n",
    "        reconstruction_loss = loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "\n",
    "        # Backprop\n",
    "        loss = reconstruction_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "torch.save(model.state_dict(), \"CVAE_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c651f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3)\n",
    "print(torch.cat((x, x), 0).shape)\n",
    "print(torch.cat((x, x), 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44e5bab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.one_hot(next(iter(train_loader))[1], num_classes=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af997a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
